{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuprotsyk/bigdata-course/blob/main/notebooks/topic02_rdd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa30c432",
      "metadata": {
        "id": "fa30c432"
      },
      "source": [
        "# Аналіз та обробка великих даних\n",
        "\n",
        "Ю.С. Процик. Курс лекцій"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af1f12bb",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "af1f12bb"
      },
      "source": [
        "# Тема 2. Основи RDD: Низькорівневе керування даними та обчисленнями в Apache Spark\n",
        "\n",
        "### План\n",
        "\n",
        "1. [Налаштування `SparkContext`](#1.-Налаштування-`SparkContext`)\n",
        "2. [Створення RDD](#2.-Створення-RDD)\n",
        "3. [Операції з RDD: Трансформації та Дії](#3.-Операції-з-RDD:-Трансформації-та-Дії)\n",
        "4. [Pair RDD та Shuffle](#4.-Pair-RDD-та-Shuffle)\n",
        "5. [Спільні змінні (Shared Variables)](#5.-Спільні-змінні-\\(Shared-Variables\\))\n",
        "6. [Кешування (Persistence)](#6.-Кешування-\\(Persistence\\))\n",
        "7. [Аналіз плану виконання через `toDebugString()`](#7.-Аналіз-плану-виконання-через-`toDebugString\\(\\)`)\n",
        "8. [Комплексний приклад та ключові концепції](#8.-Комплексний-приклад-та-ключові-концепції)\n",
        "9. [Корисні ресурси](#9.Корисні-ресурси)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RDD** – це основна абстракція Spark – відмовостійка (resilient), розподілена (distributed) колекція елементів, які можна обробляти паралельно.\n",
        "\n",
        "**Ключові властивості:**\n",
        "\n",
        "- **Розподіленість:** Дані розділені на частини між вузлами кластера.\n",
        "\n",
        "- **Відмовостійкість:** RDD автоматично відновлюються у разі збоїв вузлів.\n",
        "\n",
        "- **Незмінність (Immutable):** Після створення RDD не можна змінити, лише трансформувати у новий RDD."
      ],
      "metadata": {
        "id": "9PTal36M_DFJ"
      },
      "id": "9PTal36M_DFJ"
    },
    {
      "cell_type": "markdown",
      "id": "9c5763c3",
      "metadata": {
        "tags": [],
        "id": "9c5763c3"
      },
      "source": [
        "## 1. Налаштування `SparkContext`\n",
        "\n",
        "Перш ніж працювати з RDD, необхідно ініціалізувати `SparkSession`, яка автоматично створює `SparkContext`. В середовищі Google Colab ми використовуємо локальний режим виконання (`local[*]`), у якому всі ядра процесора імітують роботу кластера."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Встановлення PySpark\n",
        "!pip install pyspark -q\n",
        "\n",
        "# 2. Імпорт SparkSession – основної точки входу в Apache Spark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Створення сесії та отримання SparkContext\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"RDD_Lecture_Notes\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "print(f\"SparkContext ініціалізовано. Версія: {sc.version}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5OGMIhgpG41",
        "outputId": "6bc54283-f6ef-4028-ab37-dc6dcbe7f8a0"
      },
      "id": "P5OGMIhgpG41",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SparkContext ініціалізовано. Версія: 4.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Створення RDD\n",
        "\n",
        "Згідно з документацією, є два способи створення RDD:\n",
        "\n",
        "1. **Паралелізація існуючих колекцій (parallelizing):** Використовується метод `parallelize()` для перетворення існуючої колекції в програмі-драйвері.\n",
        "\n",
        "2. **Зовнішні джерела даних:** Spark може створювати RDD з будь-якого джерела, що підтримується Hadoop (локальна файлова система, HDFS, S3 тощо)."
      ],
      "metadata": {
        "id": "Wxkter34pVpw"
      },
      "id": "Wxkter34pVpw"
    },
    {
      "cell_type": "code",
      "source": [
        "# Спосіб 1: Parallelize (використовуємо 4 партиції)\n",
        "data = [(\"Laptop\", 1200), (\"Mouse\", 25), (\"Keyboard\", 45), (\"Monitor\", 300)]\n",
        "rdd_from_collection = sc.parallelize(data, 4)\n",
        "\n",
        "# Спосіб 2: Text File\n",
        "# Створимо тимчасовий файл для демонстрації\n",
        "with open(\"lecture_data.txt\", \"w\") as f:\n",
        "    f.write(\"Apache Spark\\nRDD API\\nDistributed Computing\\nOptimization\")\n",
        "\n",
        "rdd_from_file = sc.textFile(\"lecture_data.txt\")\n",
        "\n",
        "print(f\"Партицій у колекції: {rdd_from_collection.getNumPartitions()}\")\n",
        "print(f\"Перший рядок файлу: {rdd_from_file.first()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHFa3-pcpiU-",
        "outputId": "31d3c515-05ec-47ae-cf42-904dfad66dcb"
      },
      "id": "YHFa3-pcpiU-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Партицій у колекції: 4\n",
            "Перший рядок файлу: Apache Spark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Parallelize – паралелізація колекцій**\n",
        "\n",
        "Метод `parallelize()` створює RDD з колекції (list, tuple тощо).\n",
        "\n",
        "**Синтаксис:**\n",
        "```python\n",
        "sc.parallelize(data, numSlices=None)\n",
        "```\n",
        "\n",
        "**Параметри:**\n",
        "- `data` – колекція даних (list, range, tuple)\n",
        "- `numSlices` – кількість партицій (за замовчуванням = `spark.default.parallelism`)\n",
        "\n",
        "#### Що відбувається всередині?\n",
        "\n",
        "1. Driver отримує колекцію\n",
        "2. Розділяє на N партицій (slices)\n",
        "3. Розподіляє партиції по executors\n",
        "4. Кожен executor обробляє свої партиції паралельно\n",
        "\n",
        "#### Вибір кількості партицій\n",
        "\n",
        "**Емпіричне правило:** `numSlices = 2-4 × кількість CPU cores`"
      ],
      "metadata": {
        "id": "aJpHoxEajz5f"
      },
      "id": "aJpHoxEajz5f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **External Datasets – читання зовнішніх даних**\n",
        "\n",
        "Spark підтримує читання даних з різних джерел:\n",
        "\n",
        "#### Текстові файли\n",
        "```python\n",
        "sc.textFile(path, minPartitions=None)\n",
        "```\n",
        "\n",
        "**Особливості:**\n",
        "- Кожен рядок файлу → один елемент RDD\n",
        "- Підтримує wildcards: `data/*.txt`\n",
        "- Підтримує стиснуті файли: `.gz`, `.bz2`\n",
        "- Автоматично визначає кількість партицій на основі HDFS блоків\n",
        "\n",
        "**Приклади:**\n",
        "```python\n",
        "sc.textFile(\"file.txt\")              # Локальний файл\n",
        "sc.textFile(\"hdfs://path/data.txt\")  # HDFS\n",
        "sc.textFile(\"s3://bucket/data.txt\")  # S3\n",
        "sc.textFile(\"logs/*.log\")            # Wildcards\n",
        "sc.textFile(\"data.gz\")               # Стиснутий файл\n",
        "```"
      ],
      "metadata": {
        "id": "H4A5kyNalSua"
      },
      "id": "H4A5kyNalSua"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Цілі файли (wholeTextFiles)\n",
        "```python\n",
        "sc.wholeTextFiles(path)\n",
        "```\n",
        "\n",
        "Повертає `RDD[(filename, content)]` – весь файл як один запис.\n",
        "\n",
        "**Використання:** коли потрібно обробляти файл цілком (наприклад, XML, малі JSON файли).\n",
        "\n",
        "#### Інші формати\n",
        "\n",
        "**Hadoop формати:**\n",
        "```python\n",
        "sc.sequenceFile(path)           # Hadoop SequenceFile\n",
        "sc.newAPIHadoopFile(path)       # Користувацький InputFormat\n",
        "```\n",
        "\n",
        "**Сучасні структуровані формати:**\n",
        "- **JSON, CSV, Parquet** → рекомендується DataFrame API\n",
        "```python\n",
        "  df = spark.read.json(\"data.json\")\n",
        "  rdd = df.rdd  # Конвертація у разі потреби\n",
        "```\n",
        "\n",
        "**Для структурованих даних краще використовувати DataFrame API з подальшою конвертацією в RDD за потреби.**"
      ],
      "metadata": {
        "id": "lEt5DjQh-8yh"
      },
      "id": "lEt5DjQh-8yh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Операції з RDD: Трансформації та Дії\n",
        "\n",
        "RDD підтримують два типи операцій: **Трансформації (Transformations)** та **Дії (Actions)**.\n",
        "\n",
        "\n",
        "#### Transformations (Трансформації)\n",
        "\n",
        "**Lazy operations** – створюють новий RDD з існуючого, але **не виконують обчислення одразу**.\n",
        "\n",
        "- Spark запам'ятовує послідовність трансформацій у вигляді **DAG (Directed Acyclic Graph)** – орієнтованого ациклічного графа.\n",
        "\n",
        "- Одночасно будується **граф походження (lineage)** – інформація про походження кожного RDD (які RDD та через які трансформації його створили), що дозволяє відновлювати втрачені партиції у разі збоїв.\n",
        "\n",
        "- При трансформаціях дані не обчислюються, зберігається лише план виконання та метадані про залежності.\n",
        "\n",
        "![image](https://raw.githubusercontent.com/yuprotsyk/bigdata-course/refs/heads/main/img/img0201.png)"
      ],
      "metadata": {
        "id": "OlFARQ0Jrtfq"
      },
      "id": "OlFARQ0Jrtfq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Основні трансформації:\n",
        "\n",
        "| Трансформація | Опис | Приклад |\n",
        "|---------------|------|---------|\n",
        "| `map(f)` | Застосовує функцію до кожного елемента | `rdd.map(lambda x: x * 2)` |\n",
        "| `filter(f)` | Фільтрує елементи за умовою | `rdd.filter(lambda x: x > 10)` |\n",
        "| `flatMap(f)` | Map + flatten (один → багато) | `rdd.flatMap(lambda x: x.split())` |\n",
        "| `distinct()` | Видаляє дублікати | `rdd.distinct()` |\n",
        "| `union(other)` | Об'єднання двох RDD | `rdd1.union(rdd2)` |\n",
        "| `intersection(other)` | Перетин множин | `rdd1.intersection(rdd2)` |\n",
        "| `subtract(other)` | Різниця множин | `rdd1.subtract(rdd2)` |"
      ],
      "metadata": {
        "id": "NvMCx-uuuMSK"
      },
      "id": "NvMCx-uuuMSK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Типи залежностей:\n",
        "\n",
        "**Narrow (Вузькі)** – кожна партиція дочірнього RDD залежить щонайбільше від однієї партиції батьківського RDD.\n",
        "\n",
        "- Операції: `map`, `filter`, `union`, `sample`\n",
        "- Без shuffle (швидко)\n",
        "- Pipeline-обробка\n",
        "\n",
        "**Wide (Широкі)** – кожна партиція дочірнього RDD може залежати від кількох партицій батьківського RDD.\n",
        "\n",
        "- Операції: `groupByKey`, `reduceByKey`, `join`, `distinct`\n",
        "- Потребують shuffle (повільно, дорого)\n",
        "- Створюють межі stages\n"
      ],
      "metadata": {
        "id": "b-DK-OqxuhqL"
      },
      "id": "b-DK-OqxuhqL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://raw.githubusercontent.com/yuprotsyk/bigdata-course/refs/heads/main/img/img0202.png)"
      ],
      "metadata": {
        "id": "n0GtE1vY24ci"
      },
      "id": "n0GtE1vY24ci"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Actions (Дії)\n",
        "\n",
        "**Eager operations** – запускають **виконання всього DAG** і повертають результат у driver або зберігають у файлову систему.\n",
        "\n",
        "- Spark формує **фізичний план**: розбиває DAG на stages та tasks для паралельного виконання на кластері.\n",
        "\n",
        "| Action | Опис | Повертає |\n",
        "|--------|------|----------|\n",
        "| `collect()` | Збирає всі елементи на driver | `List[T]` |\n",
        "| `count()` | Підраховує кількість елементів | `int` |\n",
        "| `first()` | Повертає перший елемент | `T` |\n",
        "| `take(n)` | Повертає перші n елементів | `List[T]` |\n",
        "| `reduce(f)` | Агрегує всі елементи | `T` |\n",
        "| `foreach(f)` | Виконує функцію для кожного | `None` |\n",
        "| `saveAsTextFile(path)` | Зберігає у файл | `None` |\n",
        "| `countByValue()` | Підраховує частоту значень | `dict` |\n",
        "\n",
        "**Увага:** `collect()` завантажує **всі дані** на driver → небезпечно для великих RDD!\n"
      ],
      "metadata": {
        "id": "PVAvjNmtwD5h"
      },
      "id": "PVAvjNmtwD5h"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://raw.githubusercontent.com/yuprotsyk/bigdata-course/refs/heads/main/img/img0203.png)"
      ],
      "metadata": {
        "id": "aH0roJr_2_D_"
      },
      "id": "aH0roJr_2_D_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Трансформації — лише будують план\n",
        "prices_rdd = rdd_from_collection.map(lambda x: (x[0], x[1] * 1.1))\n",
        "expensive_items = prices_rdd.filter(lambda x: x[1] > 100)\n",
        "\n",
        "# Action: collect (ось тут Spark запускає Job)\n",
        "print(\"Результат після Actions:\", expensive_items.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t99nwpEPwxTQ",
        "outputId": "78f3445e-f7e1-47d9-a38c-b89f0a916f28"
      },
      "id": "t99nwpEPwxTQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Результат після Actions: [('Laptop', 1320.0), ('Monitor', 330.0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Pair RDD та Shuffle\n",
        "\n",
        "Деякі операції доступні лише для RDD, що складаються з кортежів `(key, value)`. Такі RDD називаються **Pair RDD**.\n",
        "\n",
        "Класичний приклад використання Pair RDD – підрахунок кількості слів у тексті.\n",
        "\n",
        "Спеціальні операції для Pair RDD:\n",
        "- **Агрегація**: `reduceByKey`, `groupByKey`, `aggregateByKey`\n",
        "- **Об'єднання**: `join`, `leftOuterJoin`, `rightOuterJoin`\n",
        "- **Сортування**: `sortByKey`\n",
        "- **Партиціонування**: `partitionBy`"
      ],
      "metadata": {
        "id": "lbcRBpMeWVHY"
      },
      "id": "lbcRBpMeWVHY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Shuffle** – це процес **перерозподілу даних між вузлами кластера**  \n",
        "для групування або агрегації за ключем.\n",
        "\n",
        "Це **дорога операція**, оскільки вона включає:\n",
        "  - дисковий ввід/вивід (disk I/O);\n",
        "  - мережеве копіювання даних між вузлами.\n",
        "\n",
        "\n",
        "#### Коли відбувається shuffle?\n",
        "\n",
        "- `groupByKey`, `reduceByKey`, `aggregateByKey`\n",
        "- `join`, `cogroup`\n",
        "- `distinct`, `intersection`\n",
        "- `repartition`"
      ],
      "metadata": {
        "id": "NaQnVJuoABah"
      },
      "id": "NaQnVJuoABah"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Важливо:** Офіційна документація рекомендує використовувати `reduceByKey` замість `groupByKey` для економії трафіку.\n",
        "\n",
        "`reduceByKey` – оптимальний вибір (комбінує локально перед shuffle):\n",
        "```python\n",
        "# ДОБРЕ\n",
        "word_counts = words_rdd.map(lambda w: (w, 1)).reduceByKey(lambda a, b: a + b)\n",
        "```\n",
        "\n",
        "`groupByKey` – передає всі дані через shuffle:\n",
        "```python\n",
        "# ПОГАНО\n",
        "word_counts = words_rdd.map(lambda w: (w, 1)).groupByKey().mapValues(sum)\n",
        "```"
      ],
      "metadata": {
        "id": "xtJowvdn9_Dy"
      },
      "id": "xtJowvdn9_Dy"
    },
    {
      "cell_type": "code",
      "source": [
        "# Сценарій: Word Count (класика)\n",
        "text = \"spark is fast spark is cool spark is distributed\"\n",
        "words_rdd = sc.parallelize(text.split(\" \"))\n",
        "\n",
        "# Створюємо пари (слово, 1) та агрегуємо\n",
        "word_counts = words_rdd.map(lambda x: (x, 1)) \\\n",
        "                       .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "print(\"Підрахунок слів:\", word_counts.collect())\n",
        "\n",
        "# Подивимось на \"родовід\" (Lineage)\n",
        "print(\"\\nГраф походження (Lineage):\")\n",
        "print(word_counts.toDebugString().decode())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiwEMkyPqzxa",
        "outputId": "ae148cf5-7498-403d-9041-6fd2c8ad5d4c"
      },
      "id": "XiwEMkyPqzxa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Підрахунок слів: [('fast', 1), ('distributed', 1), ('spark', 3), ('is', 3), ('cool', 1)]\n",
            "\n",
            "Граф походження (Lineage):\n",
            "(2) PythonRDD[14] at collect at /tmp/ipython-input-1295917330.py:9 []\n",
            " |  MapPartitionsRDD[13] at mapPartitions at PythonRDD.scala:168 []\n",
            " |  ShuffledRDD[12] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
            " +-(2) PairwiseRDD[11] at reduceByKey at /tmp/ipython-input-1295917330.py:7 []\n",
            "    |  PythonRDD[10] at reduceByKey at /tmp/ipython-input-1295917330.py:7 []\n",
            "    |  ParallelCollectionRDD[9] at readRDDFromFile at PythonRDD.scala:297 []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Коли ви бачите в Spark **Lineage (граф походження)** для операції `reduceByKey`, ключовими є три етапи:\n",
        "\n",
        "1. **Local Combine (**`PythonRDD [ ]`**):** Spark не відправляє всі дані відразу. Він спочатку підсумовує значення для кожного слова локально на кожному вузлі. Це суттєво економить мережевий трафік.\n",
        "\n",
        "2. **Shuffle (**`ShuffledRDD [ ]`**):** **Це найдорожча операція**. Spark перерозподіляє дані між вузлами кластера так, щоб усі однакові ключі (наприклад, усі слова \"spark\") опинилися на одному фізичному сервері для фінального підрахунку.\n",
        "\n",
        "3. **Final Aggregation (**`MapPartitionsRDD [ ]`**):** На кожному вузлі проводиться остаточне додавання отриманих результатів. Після цього дані готові до видачі через `.collect()`."
      ],
      "metadata": {
        "id": "atxNfAegawL9"
      },
      "id": "atxNfAegawL9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Спільні змінні (Shared Variables)\n",
        "\n",
        "За замовчуванням, коли Spark передає функцію на executors, він серіалізує та надсилає копії всіх використаних змінних для **кожного task окремо**.\n",
        "\n",
        "**Проблема:** якщо є 100 tasks і змінна розміром 1 GB → 100 GB мережевого трафіку!\n",
        "\n",
        "Для оптимізації Spark пропонує два типи спільних змінних:\n",
        "\n",
        "- **Broadcast Variables (Широкомовні змінні)**\n",
        "\n",
        "- **Accumulators (Акумулятори)**"
      ],
      "metadata": {
        "id": "GjztZyV5Cm9G"
      },
      "id": "GjztZyV5Cm9G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Broadcast Variables (Широкомовні змінні)\n",
        "\n",
        "**Призначення:** ефективна передача read-only даних.\n",
        "\n",
        "**Як працює:**\n",
        "- Змінна надсилається **один раз на executor** (не на кожен task)\n",
        "- Всі tasks на executor використовують одну копію\n",
        "- Економія: 100 tasks на 10 executors → 10 копій замість 100\n",
        "\n",
        "**Типові випадки:**\n",
        "- Lookup таблиці (словники для перекладу кодів)\n",
        "- ML моделі для prediction\n",
        "- Конфігураційні дані"
      ],
      "metadata": {
        "id": "T9_p7nUODQgJ"
      },
      "id": "T9_p7nUODQgJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Демонстрація Broadcast Variables\n",
        "\n",
        "# Без broadcast — неефективно\n",
        "print(\"=== БЕЗ BROADCAST ===\\n\")\n",
        "\n",
        "# Словник з країнами\n",
        "country_codes = {\n",
        "    \"US\": \"United States\",\n",
        "    \"UK\": \"United Kingdom\",\n",
        "    \"DE\": \"Germany\",\n",
        "    \"FR\": \"France\",\n",
        "    \"JP\": \"Japan\"\n",
        "}\n",
        "\n",
        "# RDD з кодами країн\n",
        "codes = sc.parallelize([\"US\", \"UK\", \"DE\", \"US\", \"FR\", \"JP\", \"UK\"], 3)\n",
        "\n",
        "# Без broadcast - словник передається в кожен task\n",
        "# (не рекомендовано для великих даних)\n",
        "result_no_broadcast = codes.map(lambda code: (code, country_codes.get(code, \"Unknown\")))\n",
        "print(\"Результат без broadcast:\")\n",
        "print(result_no_broadcast.collect())\n",
        "print()\n",
        "\n",
        "# З broadcast - ефективно\n",
        "print(\"=== З BROADCAST ===\\n\")\n",
        "\n",
        "# Створення broadcast змінної\n",
        "broadcast_countries = sc.broadcast(country_codes)\n",
        "\n",
        "# Використання broadcast\n",
        "result_with_broadcast = codes.map(\n",
        "    lambda code: (code, broadcast_countries.value.get(code, \"Unknown\"))\n",
        ")\n",
        "\n",
        "print(\"Результат з broadcast:\")\n",
        "print(result_with_broadcast.collect())\n",
        "print()\n",
        "\n",
        "# Статистика\n",
        "print(f\"Розмір broadcast: {len(broadcast_countries.value)} записів\")\n",
        "print(f\"Кількість партицій RDD: {codes.getNumPartitions()}\")\n",
        "print(f\"Переваги: словник передано 1 раз на executor замість N разів на task\")\n",
        "print()\n",
        "\n",
        "# Видалення broadcast змінної з пам'яті\n",
        "broadcast_countries.unpersist()\n",
        "print(\"Broadcast variable unpersisted\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjgSHDMPqbwf",
        "outputId": "178815dd-d79c-4ff9-ce27-6c9fc35e9990"
      },
      "id": "HjgSHDMPqbwf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== БЕЗ BROADCAST ===\n",
            "\n",
            "Результат без broadcast:\n",
            "[('US', 'United States'), ('UK', 'United Kingdom'), ('DE', 'Germany'), ('US', 'United States'), ('FR', 'France'), ('JP', 'Japan'), ('UK', 'United Kingdom')]\n",
            "\n",
            "=== З BROADCAST ===\n",
            "\n",
            "Результат з broadcast:\n",
            "[('US', 'United States'), ('UK', 'United Kingdom'), ('DE', 'Germany'), ('US', 'United States'), ('FR', 'France'), ('JP', 'Japan'), ('UK', 'United Kingdom')]\n",
            "\n",
            "Розмір broadcast: 5 записів\n",
            "Кількість партицій RDD: 3\n",
            "Переваги: словник передано 1 раз на executor замість N разів на task\n",
            "\n",
            "Broadcast variable unpersisted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Accumulators (Акумулятори)\n",
        "\n",
        "**Призначення:** збір метрик з розподілених обчислень.\n",
        "\n",
        "**Як працює:**\n",
        "- Executors можуть лише **додавати** значення (`add()`)\n",
        "- Driver може **читати** фінальний результат\n",
        "- Гарантується коректність лише в actions (не в transformations)\n",
        "\n",
        "**Типові випадки:**\n",
        "- Підрахунок помилок під час обробки\n",
        "- Збір статистики (кількість пропущених записів)\n",
        "- Debugging та моніторинг"
      ],
      "metadata": {
        "id": "dOQzSD3bDVEH"
      },
      "id": "dOQzSD3bDVEH"
    },
    {
      "cell_type": "code",
      "source": [
        "# Демонстрація Accumulators\n",
        "\n",
        "print(\"=== ACCUMULATORS DEMO ===\\n\")\n",
        "\n",
        "# Створення акумуляторів\n",
        "error_counter = sc.accumulator(0)\n",
        "valid_counter = sc.accumulator(0)\n",
        "sum_accumulator = sc.accumulator(0)\n",
        "\n",
        "# RDD з даними (деякі некоректні)\n",
        "data = sc.parallelize([\n",
        "    \"10\", \"20\", \"invalid\", \"30\", \"40\", \"error\", \"50\", \"60\", \"bad\", \"70\"\n",
        "])\n",
        "\n",
        "# Функція обробки з використанням акумуляторів\n",
        "def process_record(value):\n",
        "    global error_counter, valid_counter, sum_accumulator\n",
        "    try:\n",
        "        num = int(value)\n",
        "        valid_counter.add(1)\n",
        "        sum_accumulator.add(num)\n",
        "        return num\n",
        "    except ValueError:\n",
        "        error_counter.add(1)\n",
        "        return None\n",
        "\n",
        "# Обробка даних через map\n",
        "# УВАГА: Використання акумуляторів у map не гарантує точності при збоях!\n",
        "# Якщо Task буде перезапущено через Lineage, значення можуть дублюватися.\n",
        "# Для 100% точності лічильників використовуйте foreach() або агрегацію.\n",
        "result = data.map(process_record).filter(lambda x: x is not None)\n",
        "\n",
        "# Запускаємо action для trigger обчислень\n",
        "result.collect()\n",
        "\n",
        "# Читання значень акумуляторів (тільки на driver!)\n",
        "print(f\"Валідних записів: {valid_counter.value}\")\n",
        "print(f\"Помилок: {error_counter.value}\")\n",
        "print(f\"Сума валідних: {sum_accumulator.value}\")\n",
        "print(f\"Середнє: {sum_accumulator.value / valid_counter.value:.2f}\")\n",
        "print()\n",
        "\n",
        "# Приклад 2: Підрахунок слів різної довжини\n",
        "short_words = sc.accumulator(0)\n",
        "medium_words = sc.accumulator(0)\n",
        "long_words = sc.accumulator(0)\n",
        "\n",
        "text = sc.parallelize([\n",
        "    \"Apache Spark is a powerful distributed computing framework\",\n",
        "    \"It provides high-level APIs for data processing\"\n",
        "])\n",
        "\n",
        "def categorize_word(word):\n",
        "    length = len(word)\n",
        "    if length < 5:\n",
        "        short_words.add(1)\n",
        "    elif length < 8:\n",
        "        medium_words.add(1)\n",
        "    else:\n",
        "        long_words.add(1)\n",
        "    return word\n",
        "\n",
        "words_categorized = text.flatMap(lambda line: line.split()).map(categorize_word)\n",
        "words_categorized.count()  # Action для trigger\n",
        "\n",
        "print(\"Категорії слів за довжиною:\")\n",
        "print(f\"   Короткі (<5): {short_words.value}\")\n",
        "print(f\"   Середні (5-7): {medium_words.value}\")\n",
        "print(f\"   Довгі (8+): {long_words.value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0o6d8H_Kqu0p",
        "outputId": "c5bfacff-0081-4ff0-925b-1eef7c50a8d6"
      },
      "id": "0o6d8H_Kqu0p",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ACCUMULATORS DEMO ===\n",
            "\n",
            "Валідних записів: 7\n",
            "Помилок: 3\n",
            "Сума валідних: 280\n",
            "Середнє: 40.00\n",
            "\n",
            "Категорії слів за довжиною:\n",
            "   Короткі (<5): 6\n",
            "   Середні (5-7): 2\n",
            "   Довгі (8+): 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. Кешування (Persistence)\n",
        "\n",
        "#### Навіщо потрібне кешування?\n",
        "\n",
        "За замовчуванням RDD **recompute** (перераховується) кожного разу при action.\n",
        "\n",
        "**Проблема без cache:**\n",
        "```python\n",
        "rdd = sc.textFile(\"file.txt\").filter(...)\n",
        "\n",
        "rdd.count()      # Обчислення 1: читає файл + filter\n",
        "rdd.collect()    # Обчислення 2: знову читає файл + filter (!)\n",
        "```\n",
        "\n",
        "**Рішення:**\n",
        "```python\n",
        "rdd = sc.textFile(\"file.txt\").filter(...).cache()\n",
        "\n",
        "rdd.count()      # Обчислення 1: читає + filter + CACHE\n",
        "rdd.collect()    # Використовує CACHE (швидко!)\n",
        "```"
      ],
      "metadata": {
        "id": "9lLw2titI5KY"
      },
      "id": "9lLw2titI5KY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Методи кешування\n",
        "\n",
        "#### `cache()`\n",
        "\n",
        "```python\n",
        "rdd.cache()  # Зберігає в пам'яті\n",
        "```\n",
        "\n",
        "Еквівалентно `persist(StorageLevel.MEMORY_ONLY)`\n",
        "\n",
        "#### `persist(storageLevel)`\n",
        "```python\n",
        "from pyspark import StorageLevel\n",
        "\n",
        "rdd.persist(StorageLevel.MEMORY_ONLY)\n",
        "rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
        "rdd.persist(StorageLevel.DISK_ONLY)\n",
        "```\n",
        "\n",
        "#### Storage Levels\n",
        "\n",
        "| Level | Опис | Використання |\n",
        "| :--- | :--- | :--- |\n",
        "| **`MEMORY_ONLY`** | Тільки RAM | Найшвидший варіант, але дані можуть не поміститись |\n",
        "| **`MEMORY_AND_DISK`** | RAM + Диск | Оптимальний баланс швидкості та надійності |\n",
        "| **`MEMORY_ONLY_SER`** | RAM (серіалізовано) | Економить місце в пам'яті, але навантажує CPU |\n",
        "| **`DISK_ONLY`** | Тільки диск | Повільно, але гарантує результат при нестачі RAM|\n",
        "| **`OFF_HEAP`** | Поза межами JVM | Зменшує тиск на Garbage Collector (GC pressure) |\n",
        "\n",
        "**Детальний опис кожного рівня та параметрів:** [Spark RDD Programming Guide - Persistence](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence)"
      ],
      "metadata": {
        "id": "F9EWZNn-z5eR"
      },
      "id": "F9EWZNn-z5eR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Коли кешувати?\n",
        "\n",
        "**Так:**\n",
        "- RDD використовується багато разів\n",
        "- Ітеративні алгоритми (ML)\n",
        "- Інтерактивна аналітика\n",
        "\n",
        "**Ні:**\n",
        "- RDD використовується лише раз\n",
        "- Дуже великі дані, що не поміщаються в пам'ять\n",
        "\n",
        "#### Політика витіснення (Eviction Policy)\n",
        "\n",
        "Spark використовує **LRU** (Least Recently Used):\n",
        "- Коли пам’ять заповнена, видаляються дані, які використовувалися найдавніше\n",
        "\n",
        "#### Очищення кешу\n",
        "```python\n",
        "rdd.unpersist()  # Видаляє з пам'яті\n",
        "```"
      ],
      "metadata": {
        "id": "xHJ0BUnt0y7F"
      },
      "id": "xHJ0BUnt0y7F"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7. Аналіз плану виконання через `toDebugString()`\n",
        "\n",
        "Оскільки Spark працює за принципом **Lazy Evaluation**, нам потрібен спосіб \"зазирнути під капот\" ще до того, як запустяться обчислення. Метод `toDebugString()` – це головний інструмент для візуалізації **lineage** у текстовому форматі.\n",
        "\n",
        "#### Навіщо аналізувати план\n",
        "\n",
        "- **Виявлення Shuffle**  \n",
        "  Ідентифікація «широких» залежностей, що сповільнюють роботу.\n",
        "\n",
        "- **Перевірка Stages**  \n",
        "  Розуміння того, як логічний DAG розбивається на фізичні етапи.\n",
        "\n",
        "- **Контроль паралелізму**  \n",
        "  Моніторинг кількості партицій на кожному кроці.\n",
        "\n",
        "- **Відстеження кешування**  \n",
        "  Перевірка наявності мітки `[Memory...]`.\n",
        "\n",
        "\n",
        "```python\n",
        "print(rdd.toDebugString().decode())\n",
        "```"
      ],
      "metadata": {
        "id": "rrMmJMTTnxyi"
      },
      "id": "rrMmJMTTnxyi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Інтерпретація виводу\n",
        "\n",
        "**Приклад виводу:**\n",
        "```\n",
        "(4) PythonRDD[10] at RDD at PythonRDD.scala:53 []\n",
        " |  MapPartitionsRDD[9] at mapPartitions at PythonRDD.scala:133 []\n",
        " |  ShuffledRDD[8] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
        " +-(4) PairwiseRDD[7] at reduceByKey at <stdin>:1 []\n",
        "    |  PythonRDD[6] at reduceByKey at <stdin>:1 []\n",
        "    |  ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262 []\n",
        "```\n",
        "\n",
        "**Ключові символи та позначення**\n",
        "\n",
        "| Символ | Значення            | Пояснення                                                                 |\n",
        "|:-------|:---------------------|:---------------------------------------------------------------------------|\n",
        "| `\\|`    | Narrow dependency   | \"Вузька\" залежність. Дані обробляються конвеєром (pipelining) в межах одного Stage |\n",
        "| `+–`   | Wide dependency     | \"Широка\" залежність (Shuffle). Межа між Stages; верхній RDD очікує дані від нижнього |\n",
        "| `(4)`  | Partitions          | Кількість паралельних завдань (tasks) на даному етапі                     |\n",
        "| `[]`   | Storage Info        | Статус кешування. Порожні дужки – не закешовано, `[Memory...]` – дані в RAM |"
      ],
      "metadata": {
        "id": "3KvkHi27O88C"
      },
      "id": "3KvkHi27O88C"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Логіка виконання в PySpark**  \n",
        "*(читати знизу вгору)*\n",
        "\n",
        "1. **ParallelCollectionRDD**  \n",
        "  Початок: створення RDD\n",
        "\n",
        "2. **PythonRDD**  \n",
        "  Виконання вашої логіки в Python-процесах  \n",
        "  (це можуть бути `map` / `filter` або локальна агрегація перед Shuffle)\n",
        "\n",
        "3. **PairwiseRDD**  \n",
        "  Створення пар \"ключ-значення\" для подальшого групування\n",
        "\n",
        "4. **--- МЕЖА STAGE (SHUFFLE) ---** (символ `+–`)\n",
        "\n",
        "5. **ShuffledRDD**  \n",
        "  Результат фізичного перемішування даних між вузлами кластера\n",
        "\n",
        "6. **MapPartitionsRDD / PythonRDD**  \n",
        "  Фінальна обробка отриманих даних та видача результату\n",
        "\n",
        "**Важливо:** Поява символу `+–` свідчить про те, що Spark змушений розірвати конвеєр обробки та виконати **Shuffle** – найдорожчу операцію.  Оптимізація Spark-задач зазвичай зводиться до **зменшення кількості таких розривів**."
      ],
      "metadata": {
        "id": "vghJ9OOPUsEm"
      },
      "id": "vghJ9OOPUsEm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8. Комплексний приклад та ключові концепції\n",
        "\n",
        "#### Практичне завдання: аналіз логів веб-сервера"
      ],
      "metadata": {
        "id": "M1OG-VlZnxCY"
      },
      "id": "M1OG-VlZnxCY"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== КОМПЛЕКСНИЙ ПРИКЛАД: АНАЛІЗ ЛОГІВ ===\\n\")\n",
        "\n",
        "# Симуляція логів веб-сервера\n",
        "logs = sc.parallelize([\n",
        "    \"192.168.1.1 - - [01/Jan/2024:12:00:00] GET /home 200 1234\",\n",
        "    \"192.168.1.2 - - [01/Jan/2024:12:01:00] GET /about 200 2345\",\n",
        "    \"192.168.1.1 - - [01/Jan/2024:12:02:00] GET /home 200 1456\",\n",
        "    \"192.168.1.3 - - [01/Jan/2024:12:03:00] POST /contact 500 3456\",\n",
        "    \"192.168.1.2 - - [01/Jan/2024:12:04:00] GET /products 404 4567\",\n",
        "    \"192.168.1.1 - - [01/Jan/2024:12:05:00] GET /api/data 200 5678\",\n",
        "    \"192.168.1.4 - - [01/Jan/2024:12:06:00] GET /home 200 6789\",\n",
        "    \"192.168.1.3 - - [01/Jan/2024:12:07:00] GET /about 500 7890\",\n",
        "], 2)\n",
        "\n",
        "# Кешування для багаторазового використання\n",
        "logs.cache()\n",
        "\n",
        "import re\n",
        "\n",
        "# 1. Запити від кожної IP\n",
        "def extract_ip(log):\n",
        "    return log.split()[0]\n",
        "\n",
        "ip_counts = (logs\n",
        "    .map(lambda log: (extract_ip(log), 1))\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        "    .sortBy(lambda kv: kv[1], ascending=False))\n",
        "\n",
        "print(\"1. Запити від кожної IP:\")\n",
        "for ip, count in ip_counts.collect():\n",
        "    print(f\"   {ip:15} : {count} запитів\")\n",
        "print()\n",
        "\n",
        "# 2. Популярні URL\n",
        "def extract_url(log):\n",
        "    match = re.search(r'(GET|POST)\\s+(\\S+)', log)\n",
        "    return match.group(2) if match else None\n",
        "\n",
        "url_counts = (logs\n",
        "    .map(lambda log: (extract_url(log), 1))\n",
        "    .filter(lambda kv: kv[0] is not None)\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        "    .sortBy(lambda kv: kv[1], ascending=False))\n",
        "\n",
        "print(\"2. Популярні URL:\")\n",
        "for url, count in url_counts.collect():\n",
        "    print(f\"   {url:20} : {count} запитів\")\n",
        "print()\n",
        "\n",
        "# 3. Розподіл статус-кодів\n",
        "def extract_status(log):\n",
        "    parts = log.split()\n",
        "    return parts[-2] if len(parts) >= 2 else None\n",
        "\n",
        "status_distribution = (logs\n",
        "    .map(lambda log: (extract_status(log), 1))\n",
        "    .filter(lambda kv: kv[0] is not None)\n",
        "    .reduceByKey(lambda a, b: a + b))\n",
        "\n",
        "print(\"3. Розподіл статус-кодів:\")\n",
        "for status, count in status_distribution.collect():\n",
        "    print(f\"   HTTP {status} : {count} запитів\")\n",
        "print()\n",
        "\n",
        "# 4. Помилки (статус >= 400)\n",
        "errors = logs.filter(lambda log: int(extract_status(log)) >= 400 if extract_status(log) else False)\n",
        "print(\"4. Запити з помилками:\")\n",
        "for error in errors.collect():\n",
        "    print(f\"   {error}\")\n",
        "print()\n",
        "\n",
        "# 5. Середній розмір відповіді\n",
        "def extract_size(log):\n",
        "    parts = log.split()\n",
        "    return int(parts[-1]) if len(parts) >= 1 else 0\n",
        "\n",
        "total_size = logs.map(extract_size).reduce(lambda a, b: a + b)\n",
        "total_requests = logs.count()\n",
        "avg_size = total_size / total_requests\n",
        "\n",
        "print(\"5. Статистика розміру відповідей:\")\n",
        "print(f\"   Загальний розмір: {total_size} bytes\")\n",
        "print(f\"   Середній розмір: {avg_size:.2f} bytes\")\n",
        "print(f\"   Загальних запитів: {total_requests}\")\n",
        "print()\n",
        "\n",
        "# 6. Lineage graph\n",
        "print(\"6. Lineage Graph (приклад для url_counts):\")\n",
        "print(url_counts.toDebugString().decode('utf-8'))\n",
        "\n",
        "# Очищення\n",
        "logs.unpersist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJytq-k6q7d4",
        "outputId": "1ced23b8-5caf-4bf2-cd77-cdf662fbc60c"
      },
      "id": "qJytq-k6q7d4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== КОМПЛЕКСНИЙ ПРИКЛАД: АНАЛІЗ ЛОГІВ ===\n",
            "\n",
            "1. Запити від кожної IP:\n",
            "   192.168.1.1     : 3 запитів\n",
            "   192.168.1.2     : 2 запитів\n",
            "   192.168.1.3     : 2 запитів\n",
            "   192.168.1.4     : 1 запитів\n",
            "\n",
            "2. Популярні URL:\n",
            "   /home                : 3 запитів\n",
            "   /about               : 2 запитів\n",
            "   /contact             : 1 запитів\n",
            "   /products            : 1 запитів\n",
            "   /api/data            : 1 запитів\n",
            "\n",
            "3. Розподіл статус-кодів:\n",
            "   HTTP 200 : 5 запитів\n",
            "   HTTP 500 : 2 запитів\n",
            "   HTTP 404 : 1 запитів\n",
            "\n",
            "4. Запити з помилками:\n",
            "   192.168.1.3 - - [01/Jan/2024:12:03:00] POST /contact 500 3456\n",
            "   192.168.1.2 - - [01/Jan/2024:12:04:00] GET /products 404 4567\n",
            "   192.168.1.3 - - [01/Jan/2024:12:07:00] GET /about 500 7890\n",
            "\n",
            "5. Статистика розміру відповідей:\n",
            "   Загальний розмір: 33415 bytes\n",
            "   Середній розмір: 4176.88 bytes\n",
            "   Загальних запитів: 8\n",
            "\n",
            "6. Lineage Graph (приклад для url_counts):\n",
            "(2) PythonRDD[44] at collect at /tmp/ipython-input-3698269704.py:46 []\n",
            " |  MapPartitionsRDD[43] at mapPartitions at PythonRDD.scala:168 []\n",
            " |  ShuffledRDD[42] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
            " +-(2) PairwiseRDD[41] at sortBy at /tmp/ipython-input-3698269704.py:43 []\n",
            "    |  PythonRDD[40] at sortBy at /tmp/ipython-input-3698269704.py:43 []\n",
            "    |  MapPartitionsRDD[37] at mapPartitions at PythonRDD.scala:168 []\n",
            "    |  ShuffledRDD[36] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
            "    +-(2) PairwiseRDD[35] at reduceByKey at /tmp/ipython-input-3698269704.py:42 []\n",
            "       |  PythonRDD[34] at reduceByKey at /tmp/ipython-input-3698269704.py:42 []\n",
            "       |  ParallelCollectionRDD[22] at readRDDFromFile at PythonRDD.scala:297 []\n",
            "       |      CachedPartitions: 2; MemorySize: 412.0 B; DiskSize: 0.0 B\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[22] at readRDDFromFile at PythonRDD.scala:297"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ключові концепції\n",
        "\n",
        "1. **RDD – фундамент Spark**\n",
        "   - Розуміння RDD критично для глибокого знання Spark\n",
        "   - DataFrame/Dataset API будуються поверх RDD\n",
        "\n",
        "2. **Lazy Evaluation**\n",
        "   - Трансформації – lazy: не виконують код миттєво, а будують логічний DAG; для кожного RDD зберігається lineage – шлях трансформацій, що його створили.\n",
        "   - Actions – eager: запускають обчислення, перетворюючи логічний план у фізичний DAG.\n",
        "   - Оптимізація: Spark аналізує весь ланцюжок перед запуском, щоб уникнути зайвих дій.\n",
        "\n",
        "3. **Immutability**\n",
        "   - RDD незмінні\n",
        "   - Спрощує паралелізм та fault tolerance\n",
        "\n",
        "4. **Партиціонування важливе**\n",
        "   - Впливає на паралелізм та локальність даних\n",
        "   - Правило: 2-4 × кількість cores\n",
        "\n",
        "5. **Shuffle – дорога операція**\n",
        "   - Уникайте зайвого shuffle\n",
        "   - Використовуйте `reduceByKey` замість `groupByKey`"
      ],
      "metadata": {
        "id": "Bvwb9FDRoeRY"
      },
      "id": "Bvwb9FDRoeRY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "RDD – потужний інструмент, але для **structured data** (таблиці, JSON, Parquet) Spark пропонує **DataFrame API**:\n",
        "\n",
        "**DataFrame** = RDD + схема + автоматичні оптимізації\n",
        "\n",
        "```\n",
        "# RDD: ручна робота\n",
        "rdd.map(...).filter(...).reduceByKey(...)\n",
        "\n",
        "# DataFrame: декларативний стиль + оптимізації\n",
        "df.select(\"name\", \"age\").filter(df.age > 18).groupBy(\"country\").count()\n",
        "```\n",
        "\n",
        "\n",
        "Переваги **DataFrame**:\n",
        "\n",
        "- Декларативний API (що робити, не як)\n",
        "- Catalyst optimizer – автоматична оптимізація\n",
        "- Tungsten – ефективне виконання\n",
        "- SQL підтримка\n",
        "\n",
        "У **наступній лекції** детально розглянемо DataFrame API та його можливості."
      ],
      "metadata": {
        "id": "cy34tiM0IHaS"
      },
      "id": "cy34tiM0IHaS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Корисні ресурси\n",
        "\n",
        "1. [Офіційна документація по RDD](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
        "\n",
        "2. [PySpark API](https://spark.apache.org/docs/latest/api/python/)\n"
      ],
      "metadata": {
        "id": "kS6VuRHkbwSF"
      },
      "id": "kS6VuRHkbwSF"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}